{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75eb74dc-bb08-4f0d-a81e-bc87c4388ceb",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Question & Answering with SageMaker Jumpstart Foundation Model using LangChain and Amazon OpenSearch Serverless\n",
    "### Introduction\n",
    "Q&A assistants powered by generative AI are designed to have natural conversations and answer questions on a wide range of topics. It uses the LLM foundation model to understand questions and generate relevant and helpful responses. With generative AI capabilities, the Q&A assistant can create unique responses instead of pulling from a database of pre-written responses. Overall, the goal is to have more human-like conversations that can educate, assist and to help improve user productivity.\n",
    "\n",
    "While Q&A assistants powered by generative AI are helpful in providing assistance across general topics, they struggle in providing information / assistance that involves domain specific knowledge, such as enterprise data not exposed to the model used in the training process. In order to make the Q&A assistant understand enterprise data and to provide useful responses, 2 approaches are used in general to address the challenge:\n",
    "\n",
    "* Finetune the LLM model with enterprise data;\n",
    "* Integrate the LLM with enterprise knowledge through external databases (e.g. vector database). This approach is also referred as RAG (Retrieval Augmented Generation)\n",
    "\n",
    "Previously we showed how you could build a movie assistant AI RAG Chatbot using Knowledge Bases for Bedrock. In this lab, we are going to explore another option of building a RAG chatbot using an open source LLM (Meta Llama2) hosted in Amazon SageMaker through SageMaker Jumpstart. For knowledge base integration, we'll setup a vector database using Amazon OpenSearch Serverless and integrate with the LLM through Langchain framework.\n",
    "\n",
    "\n",
    "### Architecture\n",
    "![qna-rag](images/langchain-sagemaker-qa-rag.png)\n",
    "  \n",
    "#### Ask question\n",
    "![Question](./images/chatbot_lang.png)\n",
    "\n",
    "When the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model hosted in SageMaker\n",
    "- Get the contextual answer based on the documents retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe68ce-bb5f-4335-9a2b-929c8225afdb",
   "metadata": {},
   "source": [
    "## Usecase\n",
    "#### Dataset\n",
    "To explain this architecture pattern we are using a few documents from MovieLens dataset. These documents explain topics such as:\n",
    "- Movie synopsis.\n",
    "- Release dates\n",
    "- Cast members\n",
    "  \n",
    "\n",
    "#### Persona\n",
    "Let's assume a persona of a user who is looking for information about movies/shows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbee1c-6c65-42c6-9d89-1d5561d485b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opensearch-py==2.4.2 langchain==0.1.9 ipywidgets==8.0.4 boto3 scikit-learn matplotlib langchain_experimental -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03e09c-5bc1-4b5a-961f-deb305f14fad",
   "metadata": {},
   "source": [
    "Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2ac0f-aa80-41ae-aaec-3206ba53bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import glob\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from typing import Any, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f4bf2-4061-40a0-8d7b-79e817ce267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = str(uuid.uuid4().hex)[:5]\n",
    "vectordb_name=\"sm-llm-vector-db\"\n",
    "vector_store_name = f'{vectordb_name}-{random_id}'\n",
    "index_name = f\"{vectordb_name}-index-{random_id}\"\n",
    "encryption_policy_name = f\"{vectordb_name}-sp-{random_id}\"\n",
    "network_policy_name = f\"{vectordb_name}-np-{random_id}\"\n",
    "access_policy_name = f\"{vectordb_name}-ap-{random_id}\"\n",
    "kb_role_name = f\"{vectordb_name}-role-{random_id}\"\n",
    "knowledge_base_name = f\"{vectordb_name}-{random_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b16ed7-98fc-4387-9386-ad46626cf0c4",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In the following section, we're going to prepare our knoledge base store using Amazon OpenSearch Severless collection. The dataset is provided in the 'data' folder in this project and ready to be ingested. We'll leverage langchain framework to help us simplify the data ingestion process.\n",
    "\n",
    "The main steps for data ingestion workflow are:\n",
    "\n",
    "1. Create an opensearch serverless collection\n",
    "2. Load the documents from the data folder\n",
    "3. Create a numerical vector representation of each document using Amazon Bedrock Titan Embeddings model\n",
    "4. Create an open search index and ingest the document content and the corresponding embeddings into the collection.\n",
    "\n",
    "![Embeddings](./images/embeddings_lang.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fec2d7-dd3d-4ac9-b895-6d943a273034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opensearch_serverless_collection(vector_store_name, \n",
    "                                            index_name, \n",
    "                                            encryption_policy_name, \n",
    "                                            network_policy_name, \n",
    "                                            access_policy_name):\n",
    "    identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "    aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "    security_policy = aoss_client.create_security_policy(\n",
    "        name = encryption_policy_name,\n",
    "        policy = json.dumps(\n",
    "            {\n",
    "                'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                'ResourceType': 'collection'}],\n",
    "                'AWSOwnedKey': True\n",
    "            }),\n",
    "        type = 'encryption'\n",
    "    )\n",
    "\n",
    "    network_policy = aoss_client.create_security_policy(\n",
    "        name = network_policy_name,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                'ResourceType': 'collection'}],\n",
    "                'AllowFromPublic': True}\n",
    "            ]),\n",
    "        type = 'network'\n",
    "    )\n",
    "\n",
    "    collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "    while True:\n",
    "        status = aoss_client.list_collections(collectionFilters={'name':vector_store_name})['collectionSummaries'][0]['status']\n",
    "        if status in ('ACTIVE', 'FAILED'): break\n",
    "        time.sleep(10)\n",
    "\n",
    "    access_policy = aoss_client.create_access_policy(\n",
    "        name = access_policy_name,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateCollectionItems',\n",
    "                                'aoss:DeleteCollectionItems',\n",
    "                                'aoss:UpdateCollectionItems',\n",
    "                                'aoss:DescribeCollectionItems'],\n",
    "                            'ResourceType': 'collection'\n",
    "                        },\n",
    "                        {\n",
    "                            'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateIndex',\n",
    "                                'aoss:DeleteIndex',\n",
    "                                'aoss:UpdateIndex',\n",
    "                                'aoss:DescribeIndex',\n",
    "                                'aoss:ReadDocument',\n",
    "                                'aoss:WriteDocument'],\n",
    "                            'ResourceType': 'index'\n",
    "                        }],\n",
    "                    'Principal': [identity],\n",
    "                    'Description': 'Easy data policy'}\n",
    "            ]),\n",
    "        type = 'data'\n",
    "    )\n",
    "    collection_id = collection['createCollectionDetail']['id']\n",
    "    collection_arn = collection['createCollectionDetail']['arn']\n",
    "    host = collection['createCollectionDetail']['id'] + '.' + os.environ.get(\"AWS_DEFAULT_REGION\", None) + '.aoss.amazonaws.com'\n",
    "    time.sleep(60) # gives it enough time to create the collection and IAM to propagate completely so that we don't run into permission issues.\n",
    "    return host, collection_id, collection_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e021e-cfc2-44f4-9af4-9da291c06fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "host, collection_id, collection_arn = create_opensearch_serverless_collection(vector_store_name,\n",
    "                                                                              index_name,\n",
    "                                                                              encryption_policy_name,\n",
    "                                                                              network_policy_name,\n",
    "                                                                              access_policy_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0babc-a52f-4603-8d61-46b1b034796d",
   "metadata": {},
   "source": [
    "## Document Preparation\n",
    "After the OpenSearch Collection has been created, we are going to start prepare the documents for the ingestion process. For this lab, we'll be using the dataset provided in the `data` folder. These are a subset of the movielens dataset, formatted and prepared to work with this lab. We'll parse the text files and create [Document](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/documents/base.py) for each so that they could be ingested into the vector datastore that we created using Langchain framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88243dc0-9442-4048-b7e4-45a6437e4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for file in glob.glob(f\"data/*.txt\"): \n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    movie_id = lines[0].split(\":\")[1].strip()\n",
    "    title = lines[1].split(\":\")[1].strip()\n",
    "    genres = lines[2].split(\":\")[1].strip()\n",
    "    spoken_languages = lines[3].split(\":\")[1].strip()\n",
    "    release_date = lines[4].split(\":\")[1].strip()\n",
    "    rating = lines[5].split(\":\")[1].strip()\n",
    "    if rating == \"nan\":\n",
    "        rating = \"0\"\n",
    "    cast = lines[6].split(\":\")[1].strip()\n",
    "    overview = lines[7].split(\":\")[1].strip()\n",
    "    doc = Document(\n",
    "        page_content=f\"{''.join(lines)}\",\n",
    "        metadata={\n",
    "            \"title\" : title,\n",
    "            \"movie_id\": movie_id,\n",
    "            \"rating\": float(rating),\n",
    "            \"genres\": genres.split(\",\"),\n",
    "            \"spoken_languages\": spoken_languages.split(\",\"),\n",
    "            \"release_date\": release_date,\n",
    "            \"cast\" : cast.split(\",\")}\n",
    "        )\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a0ef3-6e70-4128-a3c5-0fee6e6b67df",
   "metadata": {},
   "source": [
    "# Langchain Integration \n",
    "<img src=\"images/langchain-logo.png\" alt=\"langchain\" style=\"width: 400px;\"/>\n",
    "LangChain is a framework for developing applications powered by LLMs. As a high level, langchain enables applications that are:\n",
    "\n",
    "* Data-aware: connect a language model to other sources of data\n",
    "* Agentic: allow a language model to interact with its environment\n",
    "\n",
    "The main advantages of using LangChain are:\n",
    "\n",
    "* Provides framework abstractions for working with language models, along with a collection of implementations for each abstraction. \n",
    "* Modular design principle promotes flexibility to use any LangChain components to build an application \n",
    "* Provides many Off-the-shelf chains that makes it easy to get started. \n",
    "\n",
    "Langchain has a robust set of features with Sagemaker support. In this lab, we'll be using the following langchain components to integrate with the LLM model deployed in SageMaker with an embeddings model using Amazon Titan embedding model to build a simple Q&A application.\n",
    "\n",
    "\n",
    "* [Langchain SageMaker Endpoint](https://python.langchain.com/docs/integrations/providers/sagemaker_endpoint)\n",
    "* [Amazon Titan embedding model](https://python.langchain.com/docs/integrations/text_embedding/bedrock) through Bedrock\n",
    "* [Langchain ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4bb77-f8ac-4845-8709-d6c76a74b477",
   "metadata": {},
   "source": [
    "## Creates documents and ingest into the opensearch serverless cluster.\n",
    "First, we iterate through the documents in the 'data' folder and create a Document object for each txt file. \n",
    "Then we feed the documents to opensearch serverless for ingestion using an `OpenSearchVectorSearch` object supported by Langchain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8ad25-d2e5-475b-9519-fedd1d79c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_credentials = boto3.Session().get_credentials() # needed for authenticating against opensearch cluster for index creation\n",
    "region = boto3.client(\"sts\").meta.region_name\n",
    "service = \"aoss\"\n",
    "auth = AWSV4SignerAuth(boto3_credentials, region, service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f44dea-36b7-4c2b-9e02-f5af0da0b38d",
   "metadata": {},
   "source": [
    "Define an embedding model and an LLM. In our example, we'll use Amazon Titan Embedding model as the embedding model, and Llama2-7b chat model hosted in Amazon SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f307b-c778-4d26-848e-13f981669dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc2d71-3168-432b-9b71-11c9d3402883",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    index_name=\"movielens-index\",\n",
    "    opensearch_url=f\"{host}:443\",\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    timeout = 100,\n",
    "    engine=\"faiss\")\n",
    "time.sleep(60) # Sleep for a short interval for the index to persist completely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7546e50c-086e-40b6-a8e3-530badd36330",
   "metadata": {},
   "source": [
    "# Vector Embeddings\n",
    "What is Vector embeddings? Vector embeddings help the search engine take a user query and return relevant topical web pages, recommend articles, correct misspelled words in the query, and suggest similar related queries that the user might find helpful.\n",
    "\n",
    "A vector embedding is a numerical representation of data that captures semantic relationships and similarities, making it possible to perform mathematical operations and comparisons on the data for various tasks like text analysis and recommendation systems.\n",
    "\n",
    "Here's an image that visualizes the semantic relationships between embeddings in the vector space.\n",
    "\n",
    "![vector_embeddings](images/vector-embedding-3d.jpeg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d2178-3e5d-4d9b-8329-2eb4977e2be0",
   "metadata": {},
   "source": [
    "As shown above, let's visualize the movies dataset and their relationships in 3d space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224e574-4f31-4fae-bcf5-46dad0f41d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3) # PCA reduces the dimension to 3 dimension without losing significant information\n",
    "embedding_docs = embeddings.embed_documents( [ x.page_content for x in docs ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1437e2-2aec-4443-8a01-d76286c92687",
   "metadata": {},
   "source": [
    "Extract titles from the document for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a77842-af24-45b7-afd1-fd517b2aa6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [ x.metadata['title'] for x in docs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fde680-88f6-4b77-a162-d90e59bdda0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dims = pca.fit_transform(embedding_docs)\n",
    "vis_dims_list = vis_dims.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03090a-69c1-42fd-b86d-bb2a0f1818e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_vector_embeddings(titles, vectors):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "\n",
    "    # Plot each sample category individually such that we can set label name.\n",
    "    for i, cat in enumerate(titles):\n",
    "        sub_matrix = np.array([vectors[i]])\n",
    "        x=sub_matrix[:,0]\n",
    "        y=sub_matrix[:, 1]\n",
    "        z=sub_matrix[:, 2]\n",
    "        colors = [cmap(i/len(titles))] * len(sub_matrix)\n",
    "        ax.scatter(x, y, zs=z, zdir='z', c=colors, label=cat)\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    ax.set_title(\"Movielens data embeddings in 3D\")\n",
    "    ax.legend(bbox_to_anchor=(1.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98f866-36cf-43af-b026-6390ff89a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_vector_embeddings(titles, vis_dims_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7929f48-e61f-448a-8c05-92a9d316d944",
   "metadata": {},
   "source": [
    "Let's add a query and visualize it in 3D space with all other movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee28d87-3c8c-4386-9998-39b7e89d416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I want to watch an action movie with friendship and murder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa912b-d3b3-479c-8eff-16c44bd0358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embeddings.embed_documents([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7104240-455f-4eaf-97ad-c1000b20bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dims_list.append(embedding)\n",
    "titles.append(\"QUERY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2734b-9756-4211-be85-7a97ca9fee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_vector_embeddings(titles, vis_dims_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764126a1-306f-42fe-b419-69d9d3699cfc",
   "metadata": {},
   "source": [
    "Since we have ingested the movie data into the vector database (Opensearwch Serverless Collection), we can start using the vector database to help us perform fast retrieval and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6e627-031a-4cf2-be0f-05bb0e74adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = vectorstore.similarity_search_with_score(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e773f74-4eae-44f8-8080-450bde52f61a",
   "metadata": {},
   "source": [
    "Prints the similarity scores for the highest matching documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c192d17b-2630-4159-ad08-68726e93c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in response:\n",
    "    print(f\"Title: {doc[0].metadata['title']}, similarity score: {doc[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ecc59-8ee2-4f09-92c4-326b69bc93b5",
   "metadata": {},
   "source": [
    "Another approach to calculate semantic similarity is by using the vectorstore as a retriever, as shown in the followig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bfb87-9b78-4d94-9ef3-15b497db7147",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_documents = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}).get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32367c-9e1e-4fa7-a743-8cebec56820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in relevant_documents:\n",
    "    print(\"=======\")\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8686d8-4efa-4954-9c63-71488da4a362",
   "metadata": {},
   "source": [
    "## Deploy open source Llama2 model from SageMaker Jumpstart\n",
    "\n",
    "Amazon SageMaker JumpStart is a machine learning (ML) hub that can help you accelerate your ML journey. \n",
    "\n",
    "With SageMaker JumpStart, you can evaluate, compare, and select Foundation models quickly based on pre-defined quality and responsibility metrics to perform tasks like article summarization and image generation. In addition, you can easily deploy them into production with the user interface or SDK. There are wide range of LLM selections available in SageMaker Jumpstart from open source such as Huggingface, or proprietary models such as AI21 Jurassic-2 model variants. \n",
    "\n",
    "Here's a quick view of some of the FM available directly from Jumpstart:\n",
    "\n",
    "![jumpstart models](images/sagemaker-jumpstart-llms.png)\n",
    "\n",
    "In the following section, we'll guide you through step by step for deploying an open source LLM (llama2-7b-chat) using SageMaker Jumpstart UI. After that, we'll use the model, along with the vector database integrated via Langchain framework to build simple chatbot. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f151e56-1be7-4761-841d-418eb77d692b",
   "metadata": {},
   "source": [
    "1. Navigate to SageMaker Jumpstart Launcher screen and select \"JumpStart\" from the left pane:\n",
    "\n",
    "<img src=\"images/sm-studio-home.png\" alt=\"sm studio launcher\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136fcf51-8d24-4523-a1e0-14a1aa2aab4c",
   "metadata": {},
   "source": [
    "2. In the search bar, search for: **llama 2 7B chat** and select the \"Llama2 7B Chat\" model (Note: not the neuron version)\n",
    "\n",
    "<img src=\"images/llama2-7b-chat-search.png\" alt=\"search llama2-7b chat\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e76fd9-f1bb-407d-8c1b-39cad2588a26",
   "metadata": {},
   "source": [
    "3. Spend a minute or two to read through the detail about this model. When you are done, click the \"Deploy\" button on the top right hand corner to initiate the model deploy process.\n",
    "\n",
    "<img src=\"images/jumpstart-llama2-deploy.png\" alt=\"sm jumpstart llama2 deploy\" style=\"width: 700px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28aadb-422c-43f6-a2b0-4c6ab08f7f68",
   "metadata": {},
   "source": [
    "4. Accept the EULA by clicking the checkbox, give a unique endpoint name (e.g. [your username]-genai-workshop-llama2-7b), then click **Deploy** in the lower right hand corner.\n",
    "\n",
    "<img src=\"images/jumpstart-deploy-details.png\" alt=\"sm jumpstart llama2 deploy detail\" style=\"width: 700px;\"/>\n",
    "\n",
    "The deployment should start within a few seconds. You should see the deployment messages as shown in the following:\n",
    "\n",
    "<img src=\"images/jumpstart-deploy-started.png\" alt=\"sm jumpstart llama2 deploy started\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "To optimize the compute, SageMaker Jumpstart integrates with SageMaker Inference Component in the LLM deployment process. This feature helps optimize the use of the compute resources, and allows for multiple models served behind a single endpoint. For more information about inference components, please refer to this [link](https://aws.amazon.com/blogs/aws/amazon-sagemaker-adds-new-inference-capabilities-to-help-reduce-foundation-model-deployment-costs-and-latency/).\n",
    "\n",
    "The following diagram shows the components of SageMaker Inference Component:\n",
    "\n",
    "<img src=\"images/sm-inference-component.png\" alt=\"sm inference component\" style=\"width: 700px;\"/>\n",
    "\n",
    "You could either check the deployment status by using the Jumpstart UI directly, or use the boto3. The deployment process could take 5-10 minutes. \n",
    "\n",
    "In the following, we are going to use boto3 client to check for status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea7ac1-beec-47d7-b557-40b682fca14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\")\n",
    "llm_endpoint_name = \"*********\" # replace the endpoint name with the one that you created via SM Jumpstart UI. For example, \"weteh-genai-workshop-llama2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92afeb-ac38-4050-809b-4d3f2a3f6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_deployment_complete(endpoint_name):\n",
    "    response = sm_client.list_inference_components(\n",
    "        SortBy='CreationTime',\n",
    "        SortOrder='Descending',\n",
    "        EndpointNameEquals=endpoint_name\n",
    "    )\n",
    "    inference_component_name = response['InferenceComponents'][0]['InferenceComponentName']\n",
    "    \n",
    "    response = sm_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    \n",
    "    inference_component_status = response['InferenceComponentStatus']\n",
    "    \n",
    "    while inference_component_status not in ['InService', 'Failed']:\n",
    "        time.sleep(10) # sleeps 10 seconds and check the status again\n",
    "        response = sm_client.describe_inference_component(\n",
    "            InferenceComponentName=inference_component_name\n",
    "        )\n",
    "        inference_component_status = response['InferenceComponentStatus']\n",
    "    print(f\"Inference Component: {inference_component_name} deployment is complete with status: {inference_component_status}\")\n",
    "    return inference_component_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3ad07-5e43-4511-81cb-4c2a0b2989d9",
   "metadata": {},
   "source": [
    "invoke the following with the endpoint name that you picked at the deployment step. The following cell is run until the deployment is completed.\n",
    "If you see the status with \"InService\", that means the model deployment is complete, and the endpoint is ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26f891-a5a2-488d-a2b9-f2d4d9bc6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_inference_component_name = wait_for_deployment_complete(endpoint_name=llm_endpoint_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdc352-d92a-4942-8284-e01921b987ee",
   "metadata": {},
   "source": [
    "## SageMaker Langchain Integration\n",
    "\n",
    "When using the SagemakerEndpoint class, one essential requirement is to provide a ContentHandler object. This object is responsible for converting inputs and outputs into the required formats for the specific LLM in use. While the default LLMContentHandler object bundled with the Langchain library serves some models effectively, it may not be universally applicable, particularly for models deployed via the HuggingFace LLM Inference Container.\n",
    "\n",
    "In the following code, we'll format the prompt to match the Llama prompt example described in this [notebook](https://github.com/facebookresearch/llama/blob/main/example_chat_completion.py):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc743ba4-b5e0-4582-b07e-e19fc88e9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "class SMLLMContentHandler(LLMContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "            input_data = json.dumps([[{\"role\" : \"system\", \"content\" : \"You are a movie assistant.\"},\n",
    "                                    {\"role\" : \"user\", \"content\" : prompt}]])\n",
    "            input_str = json.dumps({\"inputs\" : input_data, \"parameters\" : {**model_kwargs}})\n",
    "            return input_str.encode('utf-8')\n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            return response_json[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ecdfa0-4f36-473b-a21a-0a12c578e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import SagemakerEndpoint\n",
    "\n",
    "region_name = sm_client.meta.region_name\n",
    "model_params = { \n",
    "                    \"do_sample\": True,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"max_new_tokens\": 1000,\n",
    "                    \"stop\": [\"<|endoftext|>\", \"</s>\"],\n",
    "                    \"repetition_penalty\": 1.1\n",
    "               }\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=llm_endpoint_name,\n",
    "    region_name=region_name,\n",
    "    content_handler = SMLLMContentHandler(),\n",
    "    model_kwargs = model_params,\n",
    "    endpoint_kwargs = {\"InferenceComponentName\" : llm_inference_component_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810848b-fcfd-4361-9487-48815fba1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.chat_models import Llama2Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d7862-330c-40af-9f02-e2834483bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "system_template = \"\"\"Given the following context: \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer the question as truthfully as possible. Your answer must only be coming from the context given above. If the answer is not found in the given context. Say \"I don't know\"\n",
    "Your answer must be in a summary and direct in a concise manner. It's critical that you are only allowed to use the context given to you in answering the question.\n",
    "\"\"\"\n",
    "\n",
    "template_messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)\n",
    "model = Llama2Chat(llm=llm)\n",
    "\n",
    "vectorstore_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1881e-7489-4cc6-8e41-b1270b9dcebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key='answer')\n",
    "        vectorstore_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "        self.qa = ConversationalRetrievalChain.from_llm(llm=model,\n",
    "                                                      memory=memory,\n",
    "                                                      retriever=vectorstore_retriever, \n",
    "                                                      return_source_documents=True,\n",
    "                                                      combine_docs_chain_kwargs={\"prompt\": prompt_template})\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Let's chat!\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else:\n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=f\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    response = self.qa.invoke({\"question\" : prompt})\n",
    "                    result = response['answer']\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print(f\"AI: {result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You: \", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e1cd8-d2a5-491b-a962-7ceb30af1883",
   "metadata": {},
   "source": [
    "## Sample Questions\n",
    "* What's the movie \"Jumanji\" all about?\n",
    "* When was this movie released?\n",
    "* Who were the actors in this movie?\n",
    "* What other movies were released in the same year?\n",
    "* Tell me more about Woody in Toy Story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfa90a-e1e3-4fda-b6f7-7771115e021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatUX()\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25294db6-f194-4051-bf49-3d089ac25b9e",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we demonstrated how to build a Q&A chatbot using an Open Source LLM hosted in SageMaker, and a vector database using Amazon Opensearch serverless collection. We leveraged Langchain framework to integrate the vectordb and the LLM to make it a simple chatbot interface. \n",
    "\n",
    "First, we ingested the document provided in the 'data' folder into OpenSearch serverless collection. We showed the contextual similarities between the documents using their associated embeddings via visualizations. Then, we deployed an open source LLM (llama2-7b chat) model using SageMaker Jumpstart. Next, we leveraged Langchain library with ConversationRetrievalChain object to orchestrate the workflow between the user query and the LLM hosted in SageMaker. Finally, we built a simple Q&A interface that allows users to ask questions to the chatbot and get responses from the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96721fe-fe5a-4dbb-bdec-5a19583191c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
